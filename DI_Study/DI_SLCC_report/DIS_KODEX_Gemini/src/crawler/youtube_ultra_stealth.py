import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
from src.crawler.ultra_stealth import get_ultra_stealth_context, UltraStealth

class YouTubeCrawlerUltraStealth:
    def __init__(self, output_dir="data/raw/youtube"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "video_meta"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "thumbnails"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "comments"), exist_ok=True)

    async def collect_comments(self, page, video_url: str, max_comments: int = 50):
        """
        YouTube ë¹„ë””ì˜¤ ëŒ“ê¸€ ìˆ˜ì§‘ (Ultra Stealth)
        """
        try:
            print(f"   [Comments] Navigating to video page...")
            await page.goto(video_url, wait_until="domcontentloaded", timeout=30000)
            await UltraStealth.human_like_delay(2000, 4000)
            
            # í˜ì´ì§€ ì½ê¸° ì‹œë®¬ë ˆì´ì…˜
            await UltraStealth.simulate_reading(page, 2000)
            
            # ëŒ“ê¸€ ì„¹ì…˜ê¹Œì§€ ìŠ¤í¬ë¡¤
            print(f"   [Comments] Scrolling to comments section...")
            await UltraStealth.random_scroll(page, scrolls=3)
            
            # ëŒ“ê¸€ ë¡œë”© ëŒ€ê¸°
            try:
                await page.wait_for_selector("ytd-comment-thread-renderer", timeout=10000)
            except:
                print(f"   âš ï¸  No comments found or comments disabled")
                return []
            
            # ë” ë§ì€ ëŒ“ê¸€ ë¡œë“œ
            print(f"   [Comments] Loading more comments...")
            last_count = 0
            scroll_attempts = 0
            max_scroll_attempts = 10
            
            while scroll_attempts < max_scroll_attempts:
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await UltraStealth.human_like_delay(1500, 2500)
                
                comment_elements = await page.locator("ytd-comment-thread-renderer").all()
                current_count = len(comment_elements)
                
                print(f"   [Comments] Loaded {current_count} comments...")
                
                if current_count >= max_comments or current_count == last_count:
                    break
                    
                last_count = current_count
                scroll_attempts += 1
                
                # ëœë¤ ë§ˆìš°ìŠ¤ ì›€ì§ì„
                if random.random() < 0.3:
                    await UltraStealth.random_mouse_movement(page)
            
            # ëŒ“ê¸€ ì¶”ì¶œ
            print(f"   [Comments] Extracting comment data...")
            comment_elements = await page.locator("ytd-comment-thread-renderer").all()
            
            comments = []
            for i, comment_el in enumerate(comment_elements):
                if i >= max_comments:
                    break
                    
                try:
                    author = "Unknown"
                    try:
                        author_el = comment_el.locator("#author-text")
                        author = await author_el.text_content()
                        author = author.strip() if author else "Unknown"
                    except:
                        pass
                    
                    text = ""
                    try:
                        text_el = comment_el.locator("#content-text")
                        text = await text_el.text_content()
                        text = text.strip() if text else ""
                    except:
                        pass
                    
                    published = "Unknown"
                    try:
                        time_el = comment_el.locator("#published-time-text a")
                        published = await time_el.text_content()
                        published = published.strip() if published else "Unknown"
                    except:
                        pass
                    
                    likes = "0"
                    try:
                        like_el = comment_el.locator("#vote-count-middle")
                        likes = await like_el.text_content()
                        likes = likes.strip() if likes else "0"
                    except:
                        pass
                    
                    reply_count = 0
                    try:
                        reply_el = comment_el.locator("#more-replies span#text")
                        reply_text = await reply_el.text_content()
                        if reply_text and "ë‹µê¸€" in reply_text:
                            import re
                            match = re.search(r'\d+', reply_text)
                            if match:
                                reply_count = int(match.group())
                    except:
                        pass
                    
                    if text:
                        comment_data = {
                            "author": author,
                            "text": text,
                            "published": published,
                            "likes": likes,
                            "reply_count": reply_count,
                            "crawled_at": datetime.now().isoformat()
                        }
                        comments.append(comment_data)
                        
                except Exception as e:
                    print(f"   âš ï¸  Error parsing comment: {e}")
                    continue
            
            print(f"   âœ“ Collected {len(comments)} comments")
            return comments
            
        except Exception as e:
            print(f"   âŒ Error collecting comments: {e}")
            return []

    async def search_and_crawl(self, keyword: str, max_videos: int = 10, collect_comments: bool = True, max_comments_per_video: int = 50):
        """
        YouTube ê²€ìƒ‰ ë° í¬ë¡¤ë§ (Ultra Stealth)
        """
        async with async_playwright() as p:
            context, browser = await get_ultra_stealth_context(p, headless=True)
            page = await context.new_page()
            
            try:
                print(f"ğŸ” Searching YouTube for: {keyword}")
                
                # YouTube ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                search_url = f"https://www.youtube.com/results?search_query={keyword}"
                await page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ë° ì½ê¸° ì‹œë®¬ë ˆì´ì…˜
                await UltraStealth.simulate_reading(page, 3000)
                
                # ë””ë²„ê·¸ HTML ì €ì¥
                debug_path = f"{self.output_dir}/debug_youtube_ultra.html"
                with open(debug_path, "w", encoding="utf-8") as f:
                    f.write(await page.content())
                print(f"   âœ“ Saved debug HTML to {debug_path}")
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ë¹„ë””ì˜¤ ë¡œë“œ
                print(f"ğŸ“œ Scrolling to load more videos...")
                await UltraStealth.random_scroll(page, scrolls=3)
                
                # ë¹„ë””ì˜¤ ìš”ì†Œ ì¶”ì¶œ
                print(f"ğŸ¥ Extracting video elements...")
                video_elements = await page.locator("ytd-video-renderer").all()
                
                if not video_elements:
                    print(f"   âš ï¸  No video elements found with 'ytd-video-renderer'")
                    # ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„
                    video_elements = await page.locator("ytd-rich-item-renderer").all()
                    print(f"   â„¹ï¸  Found {len(video_elements)} with 'ytd-rich-item-renderer'")
                
                print(f"   âœ“ Found {len(video_elements)} video elements")
                
                results = []
                for i, video in enumerate(video_elements):
                    if i >= max_videos:
                        break
                    
                    try:
                        # ì œëª© ë° URL ì¶”ì¶œ
                        title_el = video.locator("#video-title")
                        title = await title_el.text_content()
                        url = await title_el.get_attribute("href")
                        
                        if not url:
                            continue
                            
                        full_url = f"https://www.youtube.com{url}" if url.startswith("/") else url
                        
                        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                        meta_text = ""
                        try:
                            meta_el = video.locator("#metadata-line")
                            meta_text = await meta_el.text_content()
                        except:
                            pass
                        
                        # ì±„ë„ëª… ì¶”ì¶œ
                        channel_name = ""
                        try:
                            channel_el = video.locator("#channel-info #text, #channel-name #text")
                            channel_name = await channel_el.first.text_content()
                        except:
                            pass
                        
                        video_data = {
                            "keyword": keyword,
                            "title": title.strip() if title else "",
                            "url": full_url,
                            "meta": meta_text.strip() if meta_text else "",
                            "channel": channel_name.strip() if channel_name else "",
                            "crawled_at": datetime.now().isoformat()
                        }
                        
                        # ì¸ë„¤ì¼ ë‹¤ìš´ë¡œë“œ
                        try:
                            thumb_url = await video.locator("ytd-thumbnail img, img").first.get_attribute("src")
                            if thumb_url and "data:image" not in thumb_url:
                                import requests
                                img_data = requests.get(thumb_url).content
                                video_id = url.split("v=")[-1].split("&")[0] if "v=" in url else url.split("/")[-1]
                                img_path = f"{self.output_dir}/thumbnails/{video_id}.jpg"
                                with open(img_path, "wb") as f:
                                    f.write(img_data)
                                video_data["thumbnail_path"] = img_path
                        except Exception as e:
                            print(f"   âš ï¸  Thumbnail error: {e}")

                        # ëŒ“ê¸€ ìˆ˜ì§‘
                        if collect_comments and full_url:
                            print(f"\nğŸ’¬ [{i+1}/{max_videos}] Collecting comments for: {title.strip()[:50]}...")
                            comments = await self.collect_comments(page, full_url, max_comments_per_video)
                            video_data["comments"] = comments
                            video_data["comment_count"] = len(comments)
                            
                            # ëŒ“ê¸€ ë³„ë„ ì €ì¥
                            if comments:
                                video_id = url.split("v=")[-1].split("&")[0] if "v=" in url else url.split("/")[-1]
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                comment_filename = f"{self.output_dir}/comments/{timestamp}_{video_id}_comments.json"
                                with open(comment_filename, "w", encoding="utf-8") as f:
                                    json.dump({
                                        "video_id": video_id,
                                        "video_title": title.strip(),
                                        "video_url": full_url,
                                        "keyword": keyword,
                                        "comments": comments,
                                        "crawled_at": datetime.now().isoformat()
                                    }, f, ensure_ascii=False, indent=2)
                                print(f"   âœ“ Saved {len(comments)} comments")
                            
                            # ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸°
                            await page.goto(search_url, wait_until="domcontentloaded")
                            await UltraStealth.human_like_delay(2000, 3000)

                        results.append(video_data)
                        print(f"âœ… [{i+1}/{max_videos}] {title.strip()[:60]}... ({len(comments) if collect_comments else 0} comments)")
                        
                    except Exception as e:
                        print(f"   âŒ Error parsing video {i+1}: {e}")
                        continue
                
                # ê²°ê³¼ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{self.output_dir}/video_meta/{timestamp}_{keyword}.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(results, f, ensure_ascii=False, indent=4)
                    
                print(f"\nğŸ‰ Saved {len(results)} videos to {filename}")
                
                return results
                
            except Exception as e:
                print(f"âŒ Critical error: {e}")
                import traceback
                traceback.print_exc()
                return []
            finally:
                await browser.close()

if __name__ == "__main__":
    import random
    crawler = YouTubeCrawlerUltraStealth(output_dir="/home/ubuntu/DI/DIS_Kodex1/data/raw/youtube")
    asyncio.run(crawler.search_and_crawl("KODEX ETF", max_videos=5, collect_comments=True, max_comments_per_video=30))
